{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BA2Plus Team Submission\n",
    "\n",
    "### 전체 프로세스 개요:\n",
    "\n",
    "- 자체제작한, 별도의 크롤링 패키지로 KRX/Naver 에서 가격, 거개량, 유동성, 시총 등의 데이터를 가져옴. \n",
    "    - 코드: https://github.com/jaepil-choi/korquanttools\n",
    "    - 각각의 데이터를 `.pickle` 로 저장해 본 코드에서 써먹음 \n",
    "    - pickle data를 받아올 수 있는 google drive 첨부: https://drive.google.com/drive/folders/1fR9fiogdhdktHuX_LbgBdMxfgHSkFT1y?usp=sharing\n",
    "- 데이터를 불러와 public 기간 전까지 자르고, 데이터를 조합하여 새로운 변수를 만듦\n",
    "    - 따라서 look-ahead 없음\n",
    "    - 새로운 변수: `close t-1`, `close t-3`, `close t-5`, `normalized rdv/adv`\n",
    "    - 각 세팅은 별도의 `submission_config.py` 모듈로 관리. (뒷부분 첨부)\n",
    "- 모델에 넣고 돌림 \n",
    "    - XGB + LGBM base model \n",
    "    - step 1, step 2, ... , step 15에 대해 따로 예측함 \n",
    "    - 시그널 만듦 \n",
    "- submission 형식에 맞게 변환\n",
    "    - `submission_util.py` 모듈로 형식에 맞게 변환함. (뒷부분 첨부)\n",
    "\n",
    "### 코드 실행환경 및 실행방법\n",
    "- 코드 실행환경\n",
    "    - python 3.9\n",
    "    - xgboost, sklearn, lightgbm, tqdm, pandas, numpy, catboost, statsmodels 필요\n",
    "    \n",
    "- 실행방법 (중요)\n",
    "    - .ipynb만 제출할 수 있다는 대회 제약 때문에 부득이 .py 모듈을 후반부에 첨부. 이 파일들이 있어야 코드가 돌아감. \n",
    "    - 코드를 실행하려면 drive 링크의 pickle 파일들을 받아 `/data` 폴더에 넣고, output을 넣을 `/output` 폴더도 만들어줘야 함.\n",
    "    - 또한, `/data` 폴더 내에 `train.csv`, `train_additional.csv`파일을 넣어야 함.\n",
    "    - 그리고 노트북과 같은 폴더 안에 `submission_config.py` 와 `submission_config.py`, `sample_submission.csv` 파일이 위치해야 함. \n",
    "\n",
    "\n",
    "나머지 과정은 아래 markdown 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom library\n",
    "\n",
    "import eda_util as eutil\n",
    "import submission_config as subconfig\n",
    "import submission_util as subutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: f'{x:,g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = subconfig.BASE_PATH\n",
    "DATA_PATH = subconfig.DATA_PATH\n",
    "\n",
    "OUTPUT_PATH = subconfig.OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krx_df = pd.read_csv(subconfig.krx_df_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krx_df.columns = ['date', 'code', 'name', 'volume', 'open', 'high', 'low', 'close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krx_df['date'] = pd.to_datetime(krx_df['date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df = pd.read_pickle(subconfig.return_df_PATH)\n",
    "close_df = pd.read_pickle(subconfig.adjclose_df_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_df = pd.read_pickle(subconfig.adjopen_df_PATH)\n",
    "high_df = pd.read_pickle(subconfig.adjhigh_df_PATH)\n",
    "low_df = pd.read_pickle(subconfig.adjlow_df_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## date list\n",
    "\n",
    "holidays = return_df.isnull().all(axis=1)\n",
    "tradingdays = ~holidays\n",
    "\n",
    "holidays = holidays.index[holidays]\n",
    "tradingdays = tradingdays.index[tradingdays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START = pd.to_datetime(subconfig.TRAIN_START, format='%Y-%m-%d')\n",
    "REALOS_PORTFOLIO_DATE = pd.to_datetime(subconfig.REALOS_PORTFOLIO_DATE, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradingdays = tradingdays[(tradingdays >= TRAIN_START) & (tradingdays <= REALOS_PORTFOLIO_DATE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dacon_sid_list = [ii[1:] for ii in krx_df['code'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df = return_df.loc[tradingdays, :].dropna(axis='columns', how='all')\n",
    "return_df = return_df.loc[:, dacon_sid_list]\n",
    "\n",
    "close_df = close_df.loc[tradingdays, :].dropna(axis='columns', how='all')\n",
    "close_df = close_df.loc[:, dacon_sid_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_df = open_df.loc[tradingdays, :].dropna(axis='columns', how='all')\n",
    "open_df = open_df.loc[:, dacon_sid_list]\n",
    "\n",
    "high_df = high_df.loc[tradingdays, :].dropna(axis='columns', how='all')\n",
    "high_df = high_df.loc[:, dacon_sid_list]\n",
    "\n",
    "low_df = low_df.loc[tradingdays, :].dropna(axis='columns', how='all')\n",
    "low_df = low_df.loc[:, dacon_sid_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMOS_START = subconfig.SIMOS_START\n",
    "# simOS_END = subconfig.SIMOS_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_df = pd.read_pickle(subconfig.volume_df_PATH)\n",
    "dollarvolume_df = pd.read_pickle(subconfig.dollarvolume_df_PATH)\n",
    "marketcap_df = pd.read_pickle(subconfig.marketcap_df_PATH)\n",
    "market_cat_df = pd.read_pickle(DATA_PATH / 'market_cat_df_20140101_20230730.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_df = volume_df.loc[tradingdays, :].dropna(axis='columns', how='all')\n",
    "volume_df = volume_df.loc[:, dacon_sid_list]\n",
    "\n",
    "dollarvolume_df = dollarvolume_df.loc[tradingdays, :].dropna(axis='columns', how='all')\n",
    "dollarvolume_df = dollarvolume_df.loc[:, dacon_sid_list]\n",
    "\n",
    "marketcap_df = marketcap_df.loc[tradingdays, :].dropna(axis='columns', how='all')\n",
    "marketcap_df = marketcap_df.loc[:, dacon_sid_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_cat_inrange = market_cat_df[market_cat_df['trdDd'].isin(tradingdays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KOSPI_sid_list = market_cat_inrange[market_cat_inrange['is_KOSPI'] == True]['ISU_SRT_CD'].unique()\n",
    "KOSDAQ_sid_list = market_cat_inrange[market_cat_inrange['is_KOSDAQ'] == True]['ISU_SRT_CD'].unique()\n",
    "KONEX_sid_list = market_cat_inrange[market_cat_inrange['is_KONEX'] == True]['ISU_SRT_CD'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALOS_PORTFOLIO_DATE = subconfig.REALOS_PORTFOLIO_DATE\n",
    "\n",
    "RDVADV_WINDOW = subconfig.WINDOWS['rdvadv'] # 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalized RDV/ADV signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_df = dollarvolume_df.rolling(RDVADV_WINDOW, ).mean().dropna(axis='rows', how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분모: average RDV/ADV ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_adv_s = adv_df.mean(axis='columns')\n",
    "avg_rdv_s = dollarvolume_df.iloc[RDVADV_WINDOW:, :].mean(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rdvadv_s = avg_rdv_s / avg_adv_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분자: individual RDV/ADV ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii_rdvadv_df = dollarvolume_df.iloc[RDVADV_WINDOW:, :] / adv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_rdvadv_signal_df = ii_rdvadv_df.divide(avg_rdvadv_s, axis='rows')\n",
    "normalized_rdvadv_signal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TRAIN_START = pd.to_datetime('2021-06-29', format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating my data with Insoo's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your function to calculate SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['code'] + ['return_day_' + str(i) for i in range(1, 16)])\n",
    "\n",
    "preds_df_fin_xgb = pd.DataFrame()\n",
    "smapes_df_fin_xgb = pd.DataFrame()\n",
    "\n",
    "preds_df_fin_lgbm = pd.DataFrame()\n",
    "smapes_df_fin_lgbm = pd.DataFrame()\n",
    "\n",
    "preds_df_fin_catboost = pd.DataFrame()\n",
    "smapes_df_fin_catboost = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 Ryzen 5 5600X 6 Core (CPU 12) 로 돌렸을 때 \n",
    "\n",
    "40분 가량 걸림. \n",
    "\n",
    "Windows에서 GPU 연산은 활용하기 어려움. \n",
    "\n",
    "- XGB: conda는 지원안함, Windows는 version conflict 남\n",
    "- LGBM: Linux만 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each unique stock\n",
    "for code in tqdm(dacon_sid_list):\n",
    "    \n",
    "    # Filter by stock code\n",
    "    # Note: All prices are adjusted\n",
    "    # TODO: Add normalized rdvadv signal to the columns\n",
    "\n",
    "    train_close = pd.DataFrame(\n",
    "        data={\n",
    "            'open': open_df.loc[MODEL_TRAIN_START:REALOS_PORTFOLIO_DATE, code],\n",
    "            'high': high_df.loc[MODEL_TRAIN_START:REALOS_PORTFOLIO_DATE, code],\n",
    "            'low': low_df.loc[MODEL_TRAIN_START:REALOS_PORTFOLIO_DATE, code],\n",
    "            'close': close_df.loc[MODEL_TRAIN_START:REALOS_PORTFOLIO_DATE, code],\n",
    "            'close_t-1': close_df.loc[MODEL_TRAIN_START:REALOS_PORTFOLIO_DATE, code].shift(1),\n",
    "            'close_t-3': close_df.loc[MODEL_TRAIN_START:REALOS_PORTFOLIO_DATE, code].shift(3),\n",
    "            'close_t-5': close_df.loc[MODEL_TRAIN_START:REALOS_PORTFOLIO_DATE, code].shift(5),\n",
    "            'dollarvolume': dollarvolume_df.loc[MODEL_TRAIN_START:REALOS_PORTFOLIO_DATE, code],\n",
    "            'marketcap': marketcap_df.loc[MODEL_TRAIN_START:REALOS_PORTFOLIO_DATE, code],\n",
    "            'norm_rdvadv': normalized_rdvadv_signal_df.loc[MODEL_TRAIN_START:REALOS_PORTFOLIO_DATE, code],\n",
    "        }\n",
    "        )\n",
    "    train_close = train_close.iloc[5:, :] # nan 있는 1st row 제거 \n",
    "\n",
    "    # Store original data for reference\n",
    "    original_data = train_close.copy()\n",
    "\n",
    "    # Create return columns for each day\n",
    "    returns = []\n",
    "    smapes_xgb = []\n",
    "    smapes_lgbm = []\n",
    "    smapes_catboost = []\n",
    "\n",
    "    preds_df_xgb = pd.DataFrame()\n",
    "    preds_df_lgbm = pd.DataFrame()\n",
    "    preds_df_catboost = pd.DataFrame()\n",
    "    \n",
    "    # For each day from 1 to 15\n",
    "    for day in range(1, 16):\n",
    "        # Scale data\n",
    "        X = train_close[:]\n",
    "        y = train_close['close']\n",
    "        \n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        data_scaled = scaler.fit_transform(X)\n",
    "        data_scaled2 = y\n",
    "        \n",
    "        X_train = data_scaled[:-day]\n",
    "        y_train = data_scaled2[day:]\n",
    "        X_test = data_scaled[-day]\n",
    "        \n",
    "        X_train = X_train[:int(len(X_train) * 0.9)]\n",
    "        X_val = X_train[int(len(X_train) * 0.9):]\n",
    "        y_train = y_train[:int(len(y_train) * 0.9)]\n",
    "        y_val = y_train[int(len(y_train) * 0.9):] \n",
    "\n",
    "        # Train XGBoost\n",
    "        xgb_model = XGBRegressor()\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        vals_xgb = xgb_model.predict(X_val)\n",
    "        smapes_xgb.append(smape(y_val, vals_xgb))\n",
    "        \n",
    "        preds_xgb = xgb_model.predict([data_scaled[-day]])\n",
    "        preds_df_xgb = pd.concat([preds_df_xgb, pd.DataFrame(preds_xgb)], axis = 0)\n",
    "\n",
    "        # Train LightGBM\n",
    "        lgbm_model = LGBMRegressor(verbose = -1)\n",
    "        lgbm_model.fit(X_train, y_train)\n",
    "        vals_lgbm = lgbm_model.predict(X_val)\n",
    "        smapes_lgbm.append(smape(y_val, vals_lgbm))\n",
    "        \n",
    "        preds_lgbm = lgbm_model.predict([data_scaled[-day]])\n",
    "        preds_df_lgbm = pd.concat([preds_df_lgbm, pd.DataFrame(preds_lgbm)], axis = 0)\n",
    "    \n",
    "    smapes_df_xgb = pd.DataFrame(smapes_xgb)\n",
    "    smapes_df_lgbm = pd.DataFrame(smapes_lgbm)\n",
    "    smapes_df_catboost = pd.DataFrame(smapes_catboost)\n",
    "\n",
    "    preds_df_fin_xgb = pd.concat([preds_df_fin_xgb, preds_df_xgb], axis = 1)\n",
    "    smapes_df_fin_xgb = pd.concat([smapes_df_fin_xgb, smapes_df_xgb], axis = 1)\n",
    "\n",
    "    preds_df_fin_lgbm = pd.concat([preds_df_fin_lgbm, preds_df_lgbm], axis = 1)\n",
    "    smapes_df_fin_lgbm = pd.concat([smapes_df_fin_lgbm, smapes_df_lgbm], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smapes_df_fin_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smapes_df_fin_lgbm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = np.zeros((len(smapes_df_fin_xgb), len(smapes_df_fin_xgb.columns)))\n",
    "\n",
    "for i in range(0, len(smapes_df_fin_xgb.columns)):\n",
    "    for j in range(0, len(smapes_df_fin_xgb)):\n",
    "        weights = [1 / smapes_df_fin_xgb.iloc[j:j+1, i].values[0],\n",
    "                   \n",
    "                   1 / smapes_df_fin_lgbm.iloc[j:j+1, i].values[0]]\n",
    "        \n",
    "\n",
    "        weights /= np.sum(weights) \n",
    "        \n",
    "\n",
    "        final[j][i] = weights[0] * preds_df_fin_xgb.iloc[j:j+1, i].values[0] \\\n",
    "                            + weights[1] * preds_df_fin_lgbm.iloc[j:j+1, i].values[0] \\\n",
    "                        #     + weights[2] * preds_df_fin_lgbm.iloc[j:j+1, i].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final)\n",
    "final_values = pd.DataFrame((final_df.iloc[-1] - final_df.iloc[0]) / final_df.iloc[0])\n",
    "final_values_sharpe = -pd.DataFrame(((final_df.iloc[-1] - final_df.iloc[0]) / final_df.iloc[0]) / final_df.pct_change().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_values_sharpe.index = dacon_sid_list\n",
    "final_values_sharpe.columns = ['VALUE']\n",
    "final_values_sharpe.reset_index(inplace = True)\n",
    "final_values_sharpe.columns = ['종목코드', 'VALUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_values_sharpe.set_index('종목코드', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_values_sharpe['VALUE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_feat_insoo = subutil.Submission(\n",
    "    alpha_series=final_values_sharpe['VALUE'],\n",
    "    alpha_name='alpha_feat_insoo_lagged_ReverseSharpe-final',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_feat_insoo.get_rank(export_path=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함께 사용된 Python 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`submission_config.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "## Path configs\n",
    "\n",
    "BASE_PATH = Path('.').resolve()\n",
    "DATA_PATH = BASE_PATH / 'data'\n",
    "OUTPUT_PATH = BASE_PATH / 'output'\n",
    "\n",
    "krx_df_PATH = DATA_PATH / 'train.csv'\n",
    "return_df_PATH = DATA_PATH / 'return_20140101_20230730.pkl'\n",
    "adjclose_df_PATH = DATA_PATH / 'adjClose_20140101_20230730.pkl'\n",
    "adjhigh_df_PATH = DATA_PATH / 'adjHigh_20140101_20230730.pkl'\n",
    "adjlow_df_PATH = DATA_PATH / 'adjLow_20140101_20230730.pkl'\n",
    "adjopen_df_PATH = DATA_PATH / 'adjOpen_20140101_20230730.pkl'\n",
    "volume_df_PATH = DATA_PATH / 'volume_df_20140101_20230730.pkl'\n",
    "dollarvolume_df_PATH = DATA_PATH / 'dollarvolume_df_20140101_20230730.pkl'\n",
    "marketcap_df_PATH = DATA_PATH / 'marketcap_df_20140101_20230730.pkl'\n",
    "\n",
    "## Param configs\n",
    "\n",
    "# train (custom)\n",
    "TRAIN_START = '2021-06-01'\n",
    "\n",
    "# SimOS\n",
    "PORTFOLIO_DATE = '2023-05-30' \n",
    "SIMOS_START = '2023-05-31'\n",
    "SIMOS_END = '2023-06-21'\n",
    "\n",
    "# RealOS\n",
    "REALOS_PORTFOLIO_DATE = '2023-07-28' \n",
    "REALOS_START = '2023-07-31'\n",
    "\n",
    "WINDOWS = {\n",
    "    'rdvadv': 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`submission_util.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score, \n",
    "    roc_curve, \n",
    "    auc\n",
    "    )\n",
    "# TODO: SimOS 에서의 정답을 알고있다. 그러므로 eval metric 계산할 수 있다. \n",
    "\n",
    "import submission_config as subconfig\n",
    "\n",
    "## Params\n",
    "DACON_SID_CNT = 2000\n",
    "SIMOS_START = subconfig.SIMOS_START\n",
    "SIMOS_END = subconfig.SIMOS_END\n",
    "\n",
    "## Import data\n",
    "krx_df = pd.read_csv(subconfig.krx_df_PATH)\n",
    "adjclose_df = pd.read_pickle(subconfig.adjclose_df_PATH)\n",
    "return_df = pd.read_pickle(subconfig.return_df_PATH)\n",
    "\n",
    "def get_simos_data(return_df, adjclose_df):\n",
    "    holidays = return_df.isnull().all(axis=1)\n",
    "    tradingdays = ~holidays\n",
    "\n",
    "    holidays = holidays.index[holidays]\n",
    "    tradingdays = tradingdays.index[tradingdays]\n",
    "\n",
    "    return_df = return_df.loc[tradingdays, :]\n",
    "    adjclose_df = adjclose_df.loc[tradingdays, :]\n",
    "\n",
    "    return_df = return_df.loc[SIMOS_START:SIMOS_END, :]\n",
    "    adjclose_df = adjclose_df.loc[SIMOS_START:SIMOS_END, :]\n",
    "\n",
    "    return return_df, adjclose_df\n",
    "\n",
    "# TODO: Confusing if global variables are not capitalized\n",
    "simos_return_df, simos_adjclose_df = get_simos_data(return_df, adjclose_df) # simos period, only trading days\n",
    "\n",
    "## for filtering\n",
    "def get_tradables(adjclose_df, trading_date=subconfig.PORTFOLIO_DATE):\n",
    "    sid_list = adjclose_df.columns\n",
    "\n",
    "    notnull = adjclose_df.loc[trading_date, :].notnull()\n",
    "    notzero = adjclose_df.loc[trading_date, :] != 0\n",
    "\n",
    "    return sid_list[notnull * notzero]\n",
    "\n",
    "def is_tradables(sid_list, tradables):\n",
    "    tradables = set(tradables)\n",
    "\n",
    "    return np.array([True if sid in tradables else False for sid in sid_list])\n",
    "\n",
    "def get_daconsids(krx_df):\n",
    "    krx_df.columns = ['date', 'code', 'name', 'volume', 'open', 'high', 'low', 'close']\n",
    "    dacon_sid_list = [ii[1:] for ii in krx_df['code'].unique()] # 060310 형식으로 바꿔줌\n",
    "\n",
    "    return dacon_sid_list\n",
    "\n",
    "def is_daconsids(sid_list, daconsids):\n",
    "    daconsids = set(daconsids)\n",
    "\n",
    "    return np.array([True if sid in daconsids else False for sid in sid_list])\n",
    "\n",
    "class Submission:\n",
    "    holding_return_s = (simos_adjclose_df.loc[SIMOS_END, :] - simos_adjclose_df.loc[SIMOS_START, :]).divide(simos_adjclose_df.loc[SIMOS_START, :])  \n",
    "    holding_return_s = holding_return_s.fillna(0)\n",
    "\n",
    "    # simos_winners = \n",
    "    # TODO: Add data science evaluation metrics\n",
    "\n",
    "    # TODO: Make not-instance-specific variables to class variables\n",
    "    def __init__(self, alpha_series:pd.Series, alpha_name:str, top=200, bottom=200):\n",
    "        self.alpha_series = alpha_series\n",
    "        self.alpha_name = alpha_name\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "\n",
    "        self.sid_list = self.alpha_series.index\n",
    "        self.tradables = get_tradables(adjclose_df)\n",
    "        self.daconsids = get_daconsids(krx_df)\n",
    "    \n",
    "        self.is_selectables = is_tradables(self.sid_list, self.tradables) * is_daconsids(self.sid_list, self.daconsids)\n",
    "        self.submission_df = None\n",
    "        self.alpha_winners = None\n",
    "        self.alpha_losers = None\n",
    "\n",
    "        # for excess return\n",
    "        self.long_hpr = None\n",
    "        self.short_hpr = None\n",
    "        self.final_return = None\n",
    "\n",
    "        # for variance\n",
    "        self.long_returns = None\n",
    "        self.short_returns = None\n",
    "        \n",
    "    def get_rank(self, export_path=None):\n",
    "        selectables = self.alpha_series[self.is_selectables]\n",
    "        top_s = selectables.nlargest(self.top)\n",
    "        bottom_s = selectables.nsmallest(self.bottom)\n",
    "        \n",
    "        self.alpha_winners = top_s.index\n",
    "        self.alpha_losers = bottom_s.index\n",
    "        \n",
    "        submission_df = pd.DataFrame(\n",
    "            data={'rank': [-1]*DACON_SID_CNT},\n",
    "            index=self.daconsids\n",
    "        )\n",
    "        submission_df.index.name = 'sid'\n",
    "\n",
    "        submission_df['rank'][top_s.index] = np.arange(1, self.top+1)\n",
    "        submission_df['rank'][bottom_s.index] = np.arange(DACON_SID_CNT, DACON_SID_CNT - self.bottom, -1)\n",
    "\n",
    "        submission_df['rank'][submission_df['rank'] == -1] = np.arange(self.top+1, DACON_SID_CNT - self.bottom + 1)\n",
    "\n",
    "        self.submission_df = submission_df\n",
    "\n",
    "        if export_path:\n",
    "            submission_df.index = ['A' + idx for idx in submission_df.index]\n",
    "            submission_df.index.name = '종목코드'\n",
    "            submission_df.columns = ['순위']\n",
    "            submission_df.to_csv(export_path / f'{self.alpha_name}.csv', encoding='utf-8')\n",
    "            \n",
    "            print(f'Saved to {export_path / self.alpha_name}.csv')\n",
    "            return submission_df\n",
    "\n",
    "        return submission_df\n",
    "\n",
    "    def get_excess_return(self, risk_free_rate=0.035, days_of_trading=15):\n",
    "        self.long_hpr = Submission.holding_return_s[self.alpha_winners].sum()\n",
    "        self.short_hpr = Submission.holding_return_s[self.alpha_losers].sum()\n",
    "\n",
    "        self.final_return = (self.long_hpr - self.short_hpr) / 400\n",
    "\n",
    "        annualized_final_return = self.final_return * 250 / days_of_trading\n",
    "        excess_return = annualized_final_return - risk_free_rate\n",
    "\n",
    "        return excess_return\n",
    "    \n",
    "    def get_volatility(self, days_of_trading=15):\n",
    "        self.long_returns = simos_return_df.loc[:, self.alpha_winners].mean(axis=1)\n",
    "        self.short_returns = simos_return_df.loc[:, self.alpha_losers].mean(axis=1)\n",
    "\n",
    "        annualized_portfolio_returns = (self.long_returns - self.short_returns) / 2 * 250\n",
    "        annualized_mean_returns = annualized_portfolio_returns.mean()\n",
    "        \n",
    "        annualized_portfolio_volatility = np.sqrt((annualized_portfolio_returns - annualized_mean_returns).pow(2)[2:].sum() / (days_of_trading-2))\n",
    "\n",
    "        return annualized_portfolio_volatility\n",
    "\n",
    "    def get_Sharpe(self):\n",
    "        return self.get_excess_return() / self.get_volatility()\n",
    "\n",
    "    \n",
    "class Score:\n",
    "    holding_return_s = (simos_adjclose_df.loc[SIMOS_END, :] - simos_adjclose_df.loc[SIMOS_START, :]).divide(simos_adjclose_df.loc[SIMOS_START, :])  \n",
    "    holding_return_s = holding_return_s.fillna(0)\n",
    "\n",
    "    def __init__(self, submission_csv_filepath, alpha_name, top=200, bottom=200, encoding='utf-8'):\n",
    "        self.alpha_name = alpha_name\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "\n",
    "        with open(submission_csv_filepath, 'r', encoding=encoding) as f:\n",
    "            submission_df = pd.read_csv(f, index_col=0)\n",
    "        \n",
    "        submission_df.index = [idx[1:] for idx in submission_df.index]\n",
    "        submission_df.index.name = 'sid'\n",
    "        submission_df.columns = ['rank']\n",
    "\n",
    "        self.alpha_series = submission_df['rank']\n",
    "        self.sid_list = self.alpha_series.index\n",
    "\n",
    "        # TODO: Add validations\n",
    "\n",
    "        self.submission_df = None\n",
    "        self.alpha_winners = self.alpha_series.nsmallest(self.top).index\n",
    "        self.alpha_losers = self.alpha_series.nlargest(self.bottom).index\n",
    "\n",
    "        # for excess return\n",
    "        self.long_hpr = None\n",
    "        self.short_hpr = None\n",
    "        self.final_return = None\n",
    "\n",
    "        # for variance\n",
    "        self.long_returns = None\n",
    "        self.short_returns = None\n",
    "    \n",
    "    def get_excess_return(self, risk_free_rate=0.035, days_of_trading=15):\n",
    "        self.long_hpr = Score.holding_return_s[self.alpha_winners].sum()\n",
    "        self.short_hpr = Score.holding_return_s[self.alpha_losers].sum()\n",
    "\n",
    "        self.final_return = (self.long_hpr - self.short_hpr) / 400\n",
    "\n",
    "        annualized_final_return = self.final_return * 250 / days_of_trading\n",
    "        excess_return = annualized_final_return - risk_free_rate\n",
    "\n",
    "        return excess_return\n",
    "\n",
    "    def get_volatility(self, days_of_trading=15):\n",
    "        self.long_returns = simos_return_df.loc[:, self.alpha_winners].mean(axis=1)\n",
    "        self.short_returns = simos_return_df.loc[:, self.alpha_losers].mean(axis=1)\n",
    "\n",
    "        annualized_portfolio_returns = (self.long_returns - self.short_returns) / 2 * 250\n",
    "        annualized_mean_returns = annualized_portfolio_returns.mean()\n",
    "        \n",
    "        annualized_portfolio_volatility = np.sqrt((annualized_portfolio_returns - annualized_mean_returns).pow(2)[2:].sum() / (days_of_trading-2))\n",
    "\n",
    "        return annualized_portfolio_volatility\n",
    "\n",
    "    def get_Sharpe(self):\n",
    "        sharpe = self.get_excess_return() / self.get_volatility()\n",
    "        print(f'Sharpe of {self.alpha_name}: {sharpe}')\n",
    "\n",
    "        return sharpe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
